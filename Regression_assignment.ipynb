{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F580mGvNoXC1"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques -1. What is Simple Linear Regression?\n",
        "\n",
        "-->  Simple Linear Regression is a fundamental statistical method used to model the relationship between two variables: one dependent (Y) and one independent (X). The goal is to find the best-fitting straight line that describes how the dependent variable changes with respect to the independent variable.\n",
        "\n",
        "The equation for simple linear regression is:\n",
        "\n",
        "ð‘Œ  = ð‘šð‘‹ + ð‘\n",
        "\n",
        "Where:\n",
        "\n",
        "Y = Dependent variable (what we want to predict)\n",
        "\n",
        "X = Independent variable (predictor)\n",
        "\n",
        "m = Slope of the regression line (shows how much Y changes with X)\n",
        "\n",
        "b = Intercept (value of Y when X = 0)"
      ],
      "metadata": {
        "id": "Ze_vNLLzoorp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques -2. What are the key assumptions of Simple Linear Regression?\n",
        "\n",
        "-->  Key Assumptions:\n",
        "\n",
        "Linearity â€“ The relationship between X and Y is linear.\n",
        "\n",
        "Independence â€“ Observations are independent of each other.\n",
        "\n",
        "Homoscedasticity â€“ The variance of errors is constant across X values.\n",
        "\n",
        "Normality â€“ Residuals (errors) follow a normal distribution."
      ],
      "metadata": {
        "id": "aklvDwbMpUDc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques -3.  What does the coefficient m represent in the equation Y=mX+c?\n",
        "\n",
        "-->\n",
        "        Y=mX+c\n",
        "\n",
        "        \n",
        "m = Slope of the regression line (shows how much Y changes with X)"
      ],
      "metadata": {
        "id": "TVxviBkeqvPE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques - 4. What does the intercept c represent in the equation Y=mX+c?\n",
        "\n",
        "-->\n",
        "Y=mX+c\n",
        "\n",
        "\n",
        "c = Intercept (value of Y when X = 0)"
      ],
      "metadata": {
        "id": "foM9deODsgn2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques -5. How do we calculate the slope m in Simple Linear Regression?\n",
        "\n",
        "-->  The slope\n",
        "ð‘š\n",
        " in Simple Linear Regression represents the rate of change in the dependent variable\n",
        "ð‘Œ\n",
        " for every unit increase in the independent variable\n",
        "ð‘‹\n",
        ". It is calculated using the following formula:\n",
        "\n",
        "ð‘š = âˆ‘(ð‘‹ð‘–âˆ’ð‘‹Ë‰)(ð‘Œð‘–âˆ’ð‘ŒË‰)/âˆ‘(ð‘‹ð‘–âˆ’ð‘‹Ë‰)2\n",
        "\n",
        "Where:\n",
        "\n",
        "ð‘‹ð‘– , ð‘Œð‘– are individual data points.\n",
        "\n",
        "ð‘‹Ë‰, ð‘ŒË‰ are the means of\n",
        "ð‘‹ and ð‘Œ, respectively.\n",
        "\n",
        "The numerator calculates the covariance between ð‘‹ and ð‘Œ.\n",
        "\n",
        "The denominator calculates the variance of ð‘‹."
      ],
      "metadata": {
        "id": "KNkXVqREtoLE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques -6. What is the purpose of the least squares method in Simple Linear Regression?\n",
        "\n",
        "-->  The least squares method is used in Simple Linear Regression to find the best-fitting line by minimizing the sum of the squared differences between actual and predicted values. In essence, it ensures the most accurate representation of the relationship between the independent variable\n",
        "ð‘‹ and the dependent variable ð‘Œ."
      ],
      "metadata": {
        "id": "21jsPcUaw-qa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques -7. How is the coefficient of determination (RÂ²) interpreted in Simple Linear Regression?\n",
        "\n",
        "-->  Formula for ð‘…2 :\n",
        "\n",
        "ð‘…2 = 1 âˆ’ âˆ‘(ð‘Œð‘– âˆ’ ð‘Œ^ð‘–)2/âˆ‘(ð‘Œð‘– âˆ’ ð‘ŒË‰)2\n",
        "\n",
        "Where:\n",
        "\n",
        "ð‘Œð‘– = Actual value\n",
        "\n",
        "ð‘Œ^ð‘– = Predicted value from the regression model\n",
        "\n",
        "ð‘ŒË‰ = Mean of actual values"
      ],
      "metadata": {
        "id": "SURFELhOz2Ib"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques - 8. What is Multiple Linear Regression?\n",
        "\n",
        "-->  Multiple Linear Regression (MLR) is an extension of Simple Linear Regression that models the relationship between one dependent variable and multiple independent variables. It helps understand how multiple factors influence the outcome and allows for better predictions.\n",
        "\n",
        "Equation for Multiple Linear Regression:\n",
        "\n",
        "ð‘Œ = ð‘0 + ð‘1ð‘‹1 + ð‘2ð‘‹2 + ...+ð‘ð‘›ð‘‹ð‘› + ðœ–"
      ],
      "metadata": {
        "id": "3UmrYwLWxCRs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques -9. What is the main difference between Simple and Multiple Linear Regression\n",
        "\n",
        "-->  Simple Linear Regression (SLR) models the relationship between one dependent variable (ð‘Œ) and a single independent variable (ð‘‹). It is used when only one factor influences the outcome.\n",
        "\n",
        "Multiple Linear Regression (MLR) extends this by incorporating two or more independent variables (ð‘‹1,ð‘‹2,...,ð‘‹ð‘›), enabling more accurate predictions by considering multiple contributing factors."
      ],
      "metadata": {
        "id": "PUbFzyOYx_c2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques - 10. What are the key assumptions of Multiple Linear Regression?\n",
        "\n",
        "-->  \n",
        "Linearity â€“ Relationship between ð‘‹ and ð‘Œ should be linear.\n",
        "\n",
        "Independence â€“ Observations should not be dependent on each other.\n",
        "\n",
        "Homoscedasticity â€“ The variance of residuals should remain constant.\n",
        "\n",
        "Normality of Residuals â€“ Errors should be normally distributed."
      ],
      "metadata": {
        "id": "1FFhBE20z1HX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques -11. What is heteroscedasticity, and how does it affect the results of a Multiple Linear Regression model?\n",
        "\n",
        "-->  Heteroscedasticity occurs when the variance of residuals (errors) in a Multiple Linear Regression (MLR) model is not constant across different values of the independent variables. In other words, the spread of residuals changes as the values of the predictors change, rather than remaining uniform.\n",
        "\n",
        "Effects of Heteroscedasticity on Regression Results:\n",
        "\n",
        "\n",
        "Biased Standard Errors â€“ Since heteroscedasticity violates the assumption of constant variance, it can lead to unreliable standard errors, making hypothesis tests inaccurate (such as t-tests for significance).\n",
        "\n",
        "Inefficient Estimates â€“ While regression coefficients remain unbiased, they become inefficient, meaning predictions may not be optimal.\n",
        "\n",
        "Underestimation or Overestimation of Coefficients â€“ Confidence intervals and p-values may be distorted, leading to incorrect conclusions about the significance of variables.\n",
        "\n",
        "Reduced Predictive Power â€“ Models affected by heteroscedasticity may produce less reliable predictions, particularly for values where variance is higher."
      ],
      "metadata": {
        "id": "fqRtjk8a0gtZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques -12. How can you improve a Multiple Linear Regression model with high multicollinearity?\n",
        "\n",
        "-->  Check for Multicollinearity using VIF (Variance Inflation Factor):\n",
        "\n",
        "* Calculate VIF for each independent variable.\n",
        "\n",
        "* A VIF above 5 or 10 indicates strong multicollinearity.\n",
        "\n",
        "* Identify highly correlated variables using correlation matrices.\n",
        "\n",
        "Remove Highly Correlated Predictors :\n",
        "\n",
        "* If two variables are highly correlated, consider dropping one that is less informative.\n",
        "\n",
        "* This simplifies the model while preserving predictive accuracy.\n",
        "\n",
        "Use Principal Component Analysis (PCA) :\n",
        "\n",
        "* PCA transforms correlated variables into uncorrelated principal components.\n",
        "\n",
        "* Useful when multiple predictors share similar information.\n",
        "\n",
        "Apply Ridge Regression (L2 Regularization):\n",
        "\n",
        "* Introduces a penalty on large coefficients, reducing the impact of multicollinearity.\n",
        "\n",
        "* This helps stabilize predictions by shrinking variable contributions.\n",
        "\n",
        "Use Lasso Regression (L1 Regularization) :\n",
        "\n",
        "* Lasso selects important variables by forcing some coefficients to zero.\n",
        "\n",
        "* Helps eliminate redundant predictors contributing to multicollinearity.\n",
        "\n",
        "Combine or Transform Variables:\n",
        "\n",
        "* Create interaction terms or aggregate features that reduce redundancy.\n",
        "\n",
        "Example: Instead of using height and weight, use BMI as a predictor."
      ],
      "metadata": {
        "id": "Q2oXkk5X16Kb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques -13. What are some common techniques for transforming categorical variables for use in regression models?\n",
        "\n",
        "-->  1. One-Hot Encoding\n",
        "* Converts each category into a separate binary column (0 or 1).\n",
        "\n",
        "* Useful for nominal categories (categories without order).\n",
        "\n",
        "Example: If \"Color\" has values Red, Blue, Green, it creates three columns:\n",
        "\n",
        "Red: (1 if Red, 0 otherwise)\n",
        "\n",
        "Blue: (1 if Blue, 0 otherwise)\n",
        "\n",
        "Green: (1 if Green, 0 otherwise)\n",
        "\n",
        "2. Label Encoding\n",
        "* Assigns numeric values to categories.\n",
        "\n",
        "* Useful for ordinal data (categories with an inherent order).\n",
        "\n",
        "Example: \"Education Level\" â†’ High School (1), College (2), Graduate (3).\n",
        "\n",
        "3. Binary Encoding\n",
        "* Combines label encoding and one-hot encoding.\n",
        "\n",
        "* Each category is converted into binary numbers, then spread across columns.\n",
        "\n",
        "* Useful when handling high-cardinality categorical variables.\n",
        "\n",
        "4. Frequency Encoding\n",
        "* Encodes categories based on their frequency in the dataset.\n",
        "\n",
        "Example: If \"Country\" appears 100 times, it gets value 100.\n",
        "\n",
        "* Helps retain distribution information."
      ],
      "metadata": {
        "id": "fHpbSd1H3X5I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques -14. What is the role of interaction terms in Multiple Linear Regression?\n",
        "\n",
        "--> Interaction terms in Multiple Linear Regression (MLR) allow us to capture the combined effect of two or more independent variables on the dependent variable. These terms help identify whether the relationship between an independent variable and the outcome depends on another variable."
      ],
      "metadata": {
        "id": "firHW8TA4P5M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques -15.  How can the interpretation of intercept differ between Simple and Multiple Linear Regression?\n",
        "\n",
        "-->  The interpretation of the intercept in Simple Linear Regression (SLR) and Multiple Linear Regression (MLR) differs significantly based on the number of independent variables and the complexity of the model."
      ],
      "metadata": {
        "id": "OmYoWPeW4iru"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques - 16. What is the significance of the slope in regression analysis, and how does it affect predictions?\n",
        "\n",
        "-->  The slope in regression analysis represents the rate of change of the dependent variable (\n",
        "ð‘Œ\n",
        ") with respect to the independent variable (\n",
        "ð‘‹\n",
        "). It plays a crucial role in making predictions and understanding relationships between variables.\n",
        "\n",
        "Significance of the Slope :\n",
        "Indicates Strength & Direction of Relationship\n",
        "\n",
        "A positive slope means that as ð‘‹ increases, ð‘Œ also increases.\n",
        "\n",
        "A negative slope means that as ð‘‹ increases, ð‘Œ decreases.\n",
        "\n",
        "A zero slope suggests no relationship between ð‘‹ and ð‘Œ.\n",
        "\n",
        "Helps in Predictive Analysis :\n",
        "\n",
        "The slope is used to estimate the expected change in ð‘Œ for a unit increase in ð‘‹.\n",
        "\n",
        "If the slope is large, small changes in ð‘‹ cause significant changes in ð‘Œ,making predictions more sensitive.\n",
        "\n",
        "Effect on Predictions :\n",
        "Accurate Interpretation: The slope tells us how much the dependent variable will increase/decrease per unit change in the predictor.\n",
        "\n",
        "Prediction Sensitivity: A steep slope implies dramatic changes in ð‘Œ, while a shallow slope indicates a weak relationship.\n",
        "\n",
        "Influence of Units: The slopeâ€™s meaning depends on the units used. For example, a slope of 0.5 in a salary prediction model means a $0.50 increase in salary per extra year of experience."
      ],
      "metadata": {
        "id": "UMxoJWSN486P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques -17. How does the intercept in a regression model provide context for the relationship between variables?\n",
        "\n",
        "--> The intercept in a regression model provides important context by defining the baseline value of the dependent variable when all independent variables are set to zero."
      ],
      "metadata": {
        "id": "2jm1XjEa6CpX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques -18. What are the limitations of using RÂ² as a sole measure of model performance?\n",
        "\n",
        "-->  1. Does Not Indicate Model Accuracy\n",
        "A high ð‘…2 does not guarantee accurate predictions.\n",
        "\n",
        "It only explains variation, but does not tell us how close predictions are to actual values.\n",
        "\n",
        "2. Sensitive to Outliers\n",
        "Extreme values can inflate or deflate ð‘…2, misleading model assessment.\n",
        "\n",
        "A model with a few extreme points may appear better or worse than it actually is.\n",
        "\n",
        "3. Cannot Detect Overfitting\n",
        "Higher ð‘…2 does not mean a better modelâ€”it might just be fitting noise.\n",
        "\n",
        "Adjusted ð‘…2 helps mitigate this, but external validation is needed.\n",
        "\n",
        "4. Does Not Account for Complexity\n",
        "Adding more predictors always increases ð‘…2, even if they are irrelevant.\n",
        "\n",
        "Adjusted ð‘…2 addresses this by penalizing unnecessary variables.\n",
        "\n",
        "5. Ignores Model Residuals\n",
        "ð‘…2 does not reveal whether errors follow a pattern (e.g., heteroscedasticity).\n",
        "\n",
        "Checking residual plots is essential for understanding model reliability.\n",
        "\n",
        "6. Does Not Work Well for Non-Linear Relationships\n",
        "If a relationship is non-linear,\n",
        "ð‘…2 might underestimate the explanatory power.\n",
        "\n",
        "Other metrics like RMSE (Root Mean Squared Error) or MAE (Mean Absolute Error) are often better."
      ],
      "metadata": {
        "id": "mJ6_gOT56fHv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques -19. How would you interpret a large standard error for a regression coefficient?\n",
        "\n",
        "--> 1. High Variability in Estimates :\n",
        "\n",
        "* A large standard error means the coefficient estimates fluctuate significantly across samples.\n",
        "\n",
        "* This indicates that the predictorâ€™s effect on the dependent variable is not stable.\n",
        "\n",
        "2. Possible Weak Relationship :\n",
        "\n",
        "* A high standard error may suggest that the independent variable is not strongly correlated with the dependent variable.\n",
        "\n",
        "* If the coefficient is close to zero and has a large standard error, the variable may not be an important predictor.\n",
        "\n",
        "3. Small Sample Size Issue :\n",
        "\n",
        "* If the dataset is small, coefficient estimates become less precise, leading to higher standard errors.\n",
        "\n",
        "* Increasing sample size can help reduce this uncertainty.\n",
        "\n",
        "4. Presence of Multicollinearity:\n",
        "* If predictors are highly correlated, regression struggles to determine the individual effect of each variable.\n",
        "\n",
        "* Multicollinearity inflates standard errors, making estimates less reliable.\n",
        "\n",
        "5. Impact on Hypothesis Testing:\n",
        "* A large standard error increases p-values, making it harder to conclude statistical significance.\n",
        "\n",
        "* Even if the coefficient is large, a high standard error may indicate low confidence in its true effect.\n",
        "\n",
        "6. Possible Model Specification Issues:\n",
        "\n",
        "* If important predictors are missing, existing coefficients may capture unintended variability.\n",
        "\n",
        "* Improving feature selection can help stabilize estimates."
      ],
      "metadata": {
        "id": "U5-ayxwH7QCU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques -20. How can heteroscedasticity be identified in residual plots, and why is it important to address it?\n",
        "\n",
        "--> Identifying Heteroscedasticity in Residual Plots Heteroscedasticity occurs when the variance of residuals (errors) changes across different values of the independent variable, violating the assumption of constant variance in regression models.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Biased Standard Errors: Can lead to incorrect confidence intervals and hypothesis testing results.\n",
        "\n",
        "Inefficient Estimates: While regression coefficients remain unbiased, they become inefficient, reducing model precision.\n",
        "\n",
        "Reduced Predictive Accuracy: Inconsistent variance can distort predictions for certain values of the independent variables.\n",
        "\n",
        "Invalid Statistical Inference: Traditional regression assumptions may no longer hold, affecting decision-making."
      ],
      "metadata": {
        "id": "S02clmiBHn-Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques -21. What does it mean if a Multiple Linear Regression model has a high RÂ² but low adjusted RÂ²?\n",
        "\n",
        "-->  If a Multiple Linear Regression (MLR) model has a high ð‘…2 but low adjusted ð‘…2, it likely indicates that the model includes too many predictors, some of which may not actually contribute to explaining the dependent variable."
      ],
      "metadata": {
        "id": "eDUJNgjgIOcZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques -22. Why is it important to scale variables in Multiple Linear Regression?\n",
        "\n",
        "-->  1. Prevents Numerical Instability:\n",
        "* When predictors have vastly different scales, regression calculations may become unstable, leading to incorrect coefficients or poor model performance.\n",
        "\n",
        "2. Improves Gradient-Based Optimization:\n",
        "* Scaling helps when using algorithms like Gradient Descent (common in machine learning models), ensuring faster and more stable convergence.\n",
        "\n",
        "3. Ensures Fair Comparison of Coefficients:\n",
        "* Without scaling, a variable with a larger numerical range might dominate regression coefficients, making interpretation misleading.\n",
        "\n",
        "Example: Age (range: 1-100) and Income (range: 1000-100000)â€”income may appear more influential just due to its larger numerical scale.\n",
        "\n",
        "4. Reduces Multicollinearity Issues:\n",
        "* When predictors have different scales, correlations between variables can be inflated, leading to high Variance Inflation Factor (VIF) values.\n",
        "\n",
        "* Standardizing features can mitigate this problem.\n",
        "\n",
        "5. Beneficial for Regularization Techniques:\n",
        "* Ridge Regression (L2 penalty) and Lasso Regression (L1 penalty) work better when features are scaled, ensuring regularization affects all variables proportionally."
      ],
      "metadata": {
        "id": "p_gxhzQgIyc7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques -23. What is polynomial regression?\n",
        "\n",
        "-->  Polynomial Regression is an extension of Linear Regression that models relationships between variables when the relationship is non-linear. Instead of fitting a straight line, it fits a curve by introducing polynomial terms (higher powers of the independent variable) into the regression equation."
      ],
      "metadata": {
        "id": "7dZiOF5AJy93"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques -24.  How does polynomial regression differ from linear regression?\n",
        "\n",
        "-->  Polynomial regression and linear regression are both techniques used to model relationships between dependent and independent variables. However, their fundamental difference lies in how they capture patternsâ€”linear regression assumes a straight-line relationship, while polynomial regression can model curved trends."
      ],
      "metadata": {
        "id": "tsa-HOQbKGPL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques -25. When is polynomial regression used?\n",
        "\n",
        "--> Polynomial regression is used when the relationship between the dependent variable (ð‘Œ) and the independent variable (ð‘‹) is non-linear, meaning a straight-line model (linear regression) does not adequately capture the data pattern."
      ],
      "metadata": {
        "id": "YHCLyiJ2Kl_O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques -26. What is the general equation for polynomial regression?\n",
        "\n",
        "-->  The general equation for polynomial regression extends linear regression by incorporating polynomial terms to model non-linear relationships. It takes the form:\n",
        "\n",
        "Y=b0+b1X+b2X2+b3X3+...+bnXn+ÏµY = b_0 + b_1X + b_2X^2 + b_3X^3 + ... + b_nX^n + Ïµ\\epsilon\n",
        "\n",
        "\n",
        "Where:\n",
        "\n",
        "\n",
        "â€¢\tYY = Dependent variable (target outcome)\n",
        "\n",
        "â€¢\tXX = Independent variable (predictor)\n",
        "\n",
        "â€¢\tb0b_0 = Intercept (baseline value of YY)\n",
        "\n",
        "â€¢\tb1,b2,...,bnb_1, b_2, ..., b_n = Regression coefficients for different powers of XX\n",
        "\n",
        "â€¢\tX2,X3,...,XnX^2, X^3, ..., X^n = Polynomial terms representing non-linear effects\n",
        "\n",
        "â€¢\tÏµ\\epsilon = Error term (captures variability not explained by the model)\n"
      ],
      "metadata": {
        "id": "1WfqGLxXK77x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques -27. Can polynomial regression be applied to multiple variables?\n",
        "\n",
        "-->  Yes! Polynomial regression can be extended to multiple variables, allowing for complex non-linear relationships between multiple independent variables and the dependent variable."
      ],
      "metadata": {
        "id": "s3ZIuewlLkkN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques -28. What are the limitations of polynomial regression?\n",
        "\n",
        "-->  Polynomial regression is a powerful tool for modeling non-linear relationships, but it comes with several limitations that must be carefully considered:\n",
        "\n",
        "1. Overfitting Risk:\n",
        "* As the degree of the polynomial increases, the model becomes more complex, potentially fitting noise rather than actual trends.\n",
        "\n",
        "Example: A high-degree polynomial might perfectly match training data but perform poorly on new observations.\n",
        "\n",
        "2. Extrapolation Issues:\n",
        "* Polynomial regression assumes that the pattern continues beyond the observed data.\n",
        "\n",
        "* Predictions for values outside the dataset can become highly unreliable, especially for high-degree polynomials.\n",
        "3. Increased Computational Complexity:\n",
        "* More polynomial terms mean higher dimensionality, which can slow down computation, especially in large datasets.\n",
        "\n",
        "4. Harder Interpretation:\n",
        "* Unlike linear regression, where coefficients have clear meanings (e.g., \"Increase in ð‘‹ leads to a direct effect on ð‘Œ\"), polynomial terms make interpretation more complex.\n",
        "\n",
        "5. Sensitive to Outliers:\n",
        "* Outliers can greatly distort a polynomial regression curve, leading to unrealistic predictions."
      ],
      "metadata": {
        "id": "1RQ0vESzL07c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques -29. What methods can be used to evaluate model fit when selecting the degree of a polynomial?\n",
        "\n",
        "--> 1. Residual Analysis:\n",
        "* Check Residual Plots: Residuals should be randomly scattered around zero.\n",
        "\n",
        "* Systematic Patterns indicate underfitting, while extreme fluctuations suggest overfitting.\n",
        "\n",
        "2. RÂ² and Adjusted RÂ²:\n",
        "* ð‘…2 measures how well the model explains variance.\n",
        "\n",
        "* Adjusted ð‘…2 accounts for unnecessary predictors, preventing overfitting.\n",
        "\n",
        "* A high ð‘…2 with decreasing adjusted ð‘…2 suggests the model is too complex.\n",
        "\n",
        "3. Mean Squared Error (MSE) / Root Mean Squared Error (RMSE):\n",
        "* Measures the average error magnitude.\n",
        "* Lower values indicate a better fit but must be validated across different degrees.\n",
        "\n",
        "4. Cross-Validation:\n",
        "* Split data into training and testing sets and compare performance.\n",
        "\n",
        "* If a model works well on training data but poorly on test data, itâ€™s likely overfitting.\n",
        "\n",
        "5. Akaike Information Criterion (AIC) & Bayesian Information Criterion (BIC):\n",
        "* Penalizes overly complex models while favoring better-fitting ones.\n",
        "\n",
        "* Lower AIC/BIC values indicate a better trade-off between accuracy and complexity.\n",
        "\n",
        "6. Variance Inflation Factor (VIF):\n",
        "* Helps check multicollinearity when introducing multiple polynomial terms.\n",
        "* High VIF values mean some predictors are redundant."
      ],
      "metadata": {
        "id": "URirKe5ZMqr1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques -30. Why is visualization important in polynomial regression?\n",
        "\n",
        "-->  Visualization is critical in polynomial regression because it helps assess model fit, detect issues like overfitting, and ensure that the chosen polynomial degree accurately represents the data pattern."
      ],
      "metadata": {
        "id": "hEYJsEhzOLcw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques -31. How is polynomial regression implemented in Python?\n",
        "\n",
        "-->  Polynomial regression in Python can be implemented using the scikit-learn library by transforming the independent variable(s) into higher-degree polynomial features. Below is a step-by-step approach:\n",
        "\n",
        "#1. Import Necessary Libraries:\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "\n",
        "#2. Generate Sample Data:\n",
        "\n",
        " Creating a dataset with a non-linear relationship\n",
        "\n",
        "X = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10]).reshape(-1, 1)\n",
        "\n",
        "y = np.array([2.5, 3.8, 7.2, 8.5, 10.8, 12.6, 17.1, 18.2, 20.5, 25.3])\n",
        "\n",
        "#3. Apply Polynomial Feature Transformation:\n",
        "\n",
        " Transforming the original feature into polynomial features of degree 2\n",
        "\n",
        "poly = PolynomialFeatures(degree=2)\n",
        "\n",
        "X_poly = poly.fit_transform(X)\n",
        "\n",
        "#4. Train Polynomial Regression Model\n",
        "\n",
        "Fit a linear regression model on the transformed data\n",
        "\n",
        "model = LinearRegression()\n",
        "\n",
        "model.fit(X_poly, y)\n",
        "\n",
        " Predict values\n",
        "\n",
        "y_pred = model.predict(X_poly)\n",
        "\n",
        "#5. Visualize the Results\n",
        "\n",
        "plt.scatter(X, y, color='blue', label='Actual Data')\n",
        "\n",
        "plt.plot(X, y_pred, color='red', label='Polynomial Fit')\n",
        "\n",
        "plt.xlabel('X')\n",
        "\n",
        "plt.ylabel('Y')\n",
        "\n",
        "plt.title('Polynomial Regression')\n",
        "\n",
        "plt.legend()\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "_G5hlCmmOpAb"
      }
    }
  ]
}